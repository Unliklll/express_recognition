### Discussion Content

对于地址方案，暂时没有变动，可能有一些更好的办法，先暂时不考虑。  

对于telephone的方案，主要参考该论文:  https://arxiv.org/abs/1507.05717  
An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition

#### pipeline:  
##### main advantages:  
1. It has unconstrained length of sequence  
2. it can be jointly trained with a loss function  

##### Network Architecture:  
![Paste_Image.png](http://upload-images.jianshu.io/upload_images/3623720-879d229c8f8b92df.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- - -
#### 2.1 feature sequence extraction

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/3623720-0abbf42c01a71bdb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

传统方法是将图片整个提取特征，但是这种办法就使得需要固定维度的输入，所以这里采取滑动卷积核的方式，可以自适应到任何长度的输入

#### 2.2 sequence labeling

![rnn.png](http://upload-images.jianshu.io/upload_images/3623720-46482708700bbf76.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

LSTM is directional, however, in the image-based sequence, contexts from both sides are very useful and comlementary for each other.

#### 2.3 transcription  

transcription is to find the label sequence with the highest probability conditioned on the per-frame predictions.  
lexicon-free and lexicon-based transcriptions

#### 2.3.1 probability of label sequence  
关键的一步是如何将rnn的输出转换成在label序列上的条件概率分布  

#### Temporal Classification
首先给出一种sequence到sequence的模型的输入和输出类型:  
input: $X=R^m$ 是m维的实值向量，目标空间 Z=L 就是所有的目标序列，S为样本空间，包含着很多个(x, z)这种序列对，每个$x = (x_1, x_2, \dots, x_T)$，每个$z = (z_1, z_2, \dots, z_U)$，一般U<T，输入的序列每个点代表着一个信息，所以得到的输出最多只能和输入的信息一样多，不能超过输入的信息  
CTC的目的是训练一个分类器h，$h: X ——> Z$，使得每给一个sequence输入，可以得到一个label输出  

下面如何定义loss函数就很重要了：  
**Label error rate:**

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/3623720-769ba6330f23fc4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


S' 表示的一个测试集，h(x)就是输入x，得到的输出结果，z表示label，ED表示h(x)要经过调整得到z的步数  
这是一种很自然的度量方式  
#### Connectionist Temporal Classification  
CTC网络多了一层输出，softmax layer。如果真实的label用L表示，输出结果是是一个比|L|长的序列，前|L|个元素表示相应位置是L中的元素的概率，多的元素表示是空白或者说没有Label的概率，总的概率就是这些概率求和  

输入x是一个长为T的序列，rnn定义 $N_w: R^m ——> R^n$，
$y = N_w(x)$，$y_k^t$表示在t的位置是label k的概率，这个概率定义在所有的可能label加上一个blank上面，也就是0到9这10个数字加一个空格，一共是11种可能，这个概率空间用$L'^T$表示

 ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/3623720-d6e5296245bd84c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

然后在序列$\pi$上面定义一个函数B，将序列B映射为label，比如将“--hh-e-l-ll-oo--”通过去重，去空格，映射到“hello”

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/3623720-832945e5b7d6836e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 2.4 Network Training
$X = \{ I_i, L_i\}_i$，其中$I_i$表示输入图片，$L_i$表示真实的label

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/3623720-1b74bbf2169b6a51.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

就是最小化这个函数，可以用backpropogation的方法
